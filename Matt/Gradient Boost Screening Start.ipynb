{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df = pd.read_csv('diabetes_cleaned_12-15-20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns='readmit_30d')\n",
    "y = df.readmit_30d\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'class_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bab20d54b5aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mroc_scorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mscaler_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'class_weight'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'n_estimators':[100,200]}\n",
    "estimator = GradientBoostingClassifier(random_state=5,verbose=1,class_weight='balanced')\n",
    "roc_scorer = make_scorer(roc_auc_score)\n",
    "scaler_X = StandardScaler().fit(X_train)\n",
    "scaled_X = scaler_X.transform(X_train)\n",
    "scaled_X = pd.DataFrame(scaled_X,index=X_train.index)\n",
    "\n",
    "gscv = GridSearchCV(estimator=estimator,param_grid=params,scoring=roc_scorer,cv=5,verbose=1,n_jobs=3)\n",
    "gscv.fit(scaled_X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([13.85038304, 23.13656793]),\n",
       " 'std_fit_time': array([0.41406242, 5.5292048 ]),\n",
       " 'mean_score_time': array([0.0587851, 0.0650775]),\n",
       " 'std_score_time': array([0.00935017, 0.02447723]),\n",
       " 'param_n_estimators': masked_array(data=[100, 200],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_estimators': 100}, {'n_estimators': 200}],\n",
       " 'split0_test_score': array([0.50323727, 0.50403605]),\n",
       " 'split1_test_score': array([0.50304102, 0.50483482]),\n",
       " 'split2_test_score': array([0.50244098, 0.50428582]),\n",
       " 'split3_test_score': array([0.50174672, 0.50259554]),\n",
       " 'split4_test_score': array([0.50195029, 0.50335351]),\n",
       " 'mean_test_score': array([0.50248329, 0.50382118]),\n",
       " 'std_test_score': array([0.00058446, 0.00077594]),\n",
       " 'rank_test_score': array([2, 1], dtype=int32)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   46.3s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3828            3.94s\n",
      "         2           1.3798            3.94s\n",
      "         3           1.3770            3.99s\n",
      "         4           1.3745            3.92s\n",
      "         5           1.3722            3.92s\n",
      "         6           1.3700            3.91s\n",
      "         7           1.3681            3.88s\n",
      "         8           1.3664            3.88s\n",
      "         9           1.3644            3.85s\n",
      "        10           1.3629            3.95s\n",
      "        20           1.3501            3.89s\n",
      "        30           1.3418            4.53s\n",
      "        40           1.3356            4.24s\n",
      "        50           1.3305            3.96s\n",
      "        60           1.3265            3.72s\n",
      "        70           1.3232            3.70s\n",
      "        80           1.3204            3.51s\n",
      "        90           1.3179            3.37s\n",
      "       100           1.3156            3.30s\n",
      "       200           1.2995            2.25s\n",
      "       300           1.2892            1.45s\n",
      "       400           1.2818            0.72s\n",
      "       500           1.2749            0.00s\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3832            7.97s\n",
      "         2           1.3804            5.89s\n",
      "         3           1.3779            5.21s\n",
      "         4           1.3756            4.91s\n",
      "         5           1.3734            4.66s\n",
      "         6           1.3714            4.56s\n",
      "         7           1.3695            4.45s\n",
      "         8           1.3679            4.51s\n",
      "         9           1.3662            4.41s\n",
      "        10           1.3646            4.33s\n",
      "        20           1.3530            4.28s\n",
      "        30           1.3453            4.54s\n",
      "        40           1.3392            4.16s\n",
      "        50           1.3344            3.99s\n",
      "        60           1.3301            3.81s\n",
      "        70           1.3264            3.65s\n",
      "        80           1.3234            3.48s\n",
      "        90           1.3208            3.34s\n",
      "       100           1.3185            3.27s\n",
      "       200           1.3034            2.26s\n",
      "       300           1.2942            1.46s\n",
      "       400           1.2869            0.72s\n",
      "       500           1.2812            0.00s\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   42.8s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3815            6.42s\n",
      "         2           1.3773            6.36s\n",
      "         3           1.3736            6.72s\n",
      "         4           1.3701            6.86s\n",
      "         5           1.3670            7.03s\n",
      "         6           1.3641            7.10s\n",
      "         7           1.3611            7.04s\n",
      "         8           1.3587            7.23s\n",
      "         9           1.3563            7.21s\n",
      "        10           1.3542            7.34s\n",
      "        20           1.3368            8.20s\n",
      "        30           1.3253            7.62s\n",
      "        40           1.3172            7.24s\n",
      "        50           1.3108            6.70s\n",
      "        60           1.3052            6.51s\n",
      "        70           1.3008            6.15s\n",
      "        80           1.2964            5.88s\n",
      "        90           1.2927            5.62s\n",
      "       100           1.2895            5.35s\n",
      "       200           1.2655            3.64s\n",
      "       300           1.2485            2.32s\n",
      "       400           1.2342            1.13s\n",
      "       500           1.2224            0.00s\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3838            3.72s\n",
      "         2           1.3815            3.69s\n",
      "         3           1.3793            3.72s\n",
      "         4           1.3772            3.73s\n",
      "         5           1.3752            3.69s\n",
      "         6           1.3733            3.69s\n",
      "         7           1.3714            3.68s\n",
      "         8           1.3697            3.74s\n",
      "         9           1.3680            3.79s\n",
      "        10           1.3664            3.84s\n",
      "        20           1.3539            5.12s\n",
      "        30           1.3442            4.62s\n",
      "        40           1.3364            4.36s\n",
      "        50           1.3303            4.12s\n",
      "        60           1.3253            3.85s\n",
      "        70           1.3210            3.52s\n",
      "        80           1.3172            3.32s\n",
      "        90           1.3136            3.08s\n",
      "       100           1.3105            2.86s\n",
      "       200           1.2891            1.26s\n",
      "       300           1.2757            0.00s\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.8s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3808            2.71s\n",
      "         2           1.3762            2.53s\n",
      "         3           1.3722            2.49s\n",
      "         4           1.3689            2.56s\n",
      "         5           1.3654            2.50s\n",
      "         6           1.3626            2.47s\n",
      "         7           1.3601            2.58s\n",
      "         8           1.3576            2.49s\n",
      "         9           1.3555            2.49s\n",
      "        10           1.3534            2.46s\n",
      "        20           1.3391            2.20s\n",
      "        30           1.3289            2.36s\n",
      "        40           1.3214            2.15s\n",
      "        50           1.3158            1.98s\n",
      "        60           1.3111            1.86s\n",
      "        70           1.3074            1.75s\n",
      "        80           1.3042            1.64s\n",
      "        90           1.3010            1.57s\n",
      "       100           1.2984            1.48s\n",
      "       200           1.2801            0.71s\n",
      "       300           1.2693            0.00s\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.2s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3826            3.94s\n",
      "         2           1.3793            3.99s\n",
      "         3           1.3764            3.87s\n",
      "         4           1.3738            3.90s\n",
      "         5           1.3714            3.85s\n",
      "         6           1.3692            4.03s\n",
      "         7           1.3670            4.01s\n",
      "         8           1.3651            4.10s\n",
      "         9           1.3631            4.04s\n",
      "        10           1.3615            4.11s\n",
      "        20           1.3481            3.98s\n",
      "        30           1.3392            3.90s\n",
      "        40           1.3327            3.75s\n",
      "        50           1.3277            3.66s\n",
      "        60           1.3236            3.49s\n",
      "        70           1.3200            3.30s\n",
      "        80           1.3169            3.15s\n",
      "        90           1.3141            3.03s\n",
      "       100           1.3116            2.94s\n",
      "       200           1.2949            2.10s\n",
      "       300           1.2855            1.36s\n",
      "       400           1.2785            0.67s\n",
      "       500           1.2723            0.00s\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3830            3.68s\n",
      "         2           1.3801            3.74s\n",
      "         3           1.3775            3.83s\n",
      "         4           1.3751            3.82s\n",
      "         5           1.3729            3.84s\n",
      "         6           1.3708            3.85s\n",
      "         7           1.3688            3.85s\n",
      "         8           1.3670            3.88s\n",
      "         9           1.3652            3.87s\n",
      "        10           1.3636            3.85s\n",
      "        20           1.3514            3.89s\n",
      "        30           1.3430            3.80s\n",
      "        40           1.3367            3.58s\n",
      "        50           1.3316            3.45s\n",
      "        60           1.3274            3.27s\n",
      "        70           1.3240            3.18s\n",
      "        80           1.3208            3.05s\n",
      "        90           1.3180            2.93s\n",
      "       100           1.3153            2.85s\n",
      "       200           1.2981            2.05s\n",
      "       300           1.2885            1.34s\n",
      "       400           1.2807            0.67s\n",
      "       500           1.2749            0.00s\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.3s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3803            3.97s\n",
      "         2           1.3755            3.94s\n",
      "         3           1.3713            3.98s\n",
      "         4           1.3679            4.18s\n",
      "         5           1.3646            4.07s\n",
      "         6           1.3620            4.23s\n",
      "         7           1.3592            4.13s\n",
      "         8           1.3566            4.10s\n",
      "         9           1.3544            4.11s\n",
      "        10           1.3522            4.09s\n",
      "        20           1.3365            4.05s\n",
      "        30           1.3264            3.88s\n",
      "        40           1.3187            3.67s\n",
      "        50           1.3130            3.46s\n",
      "        60           1.3081            3.28s\n",
      "        70           1.3041            3.22s\n",
      "        80           1.3005            3.12s\n",
      "        90           1.2978            2.98s\n",
      "       100           1.2955            2.91s\n",
      "       200           1.2775            2.08s\n",
      "       300           1.2663            1.37s\n",
      "       400           1.2581            0.69s\n",
      "       500           1.2512            0.00s\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3807            0.81s\n",
      "         2           1.3762            0.79s\n",
      "         3           1.3723            0.78s\n",
      "         4           1.3690            0.76s\n",
      "         5           1.3658            0.76s\n",
      "         6           1.3630            0.76s\n",
      "         7           1.3607            0.78s\n",
      "         8           1.3583            0.77s\n",
      "         9           1.3561            0.75s\n",
      "        10           1.3542            0.74s\n",
      "        20           1.3409            0.61s\n",
      "        30           1.3321            0.56s\n",
      "        40           1.3259            0.46s\n",
      "        50           1.3212            0.38s\n",
      "        60           1.3173            0.29s\n",
      "        70           1.3140            0.22s\n",
      "        80           1.3110            0.15s\n",
      "        90           1.3085            0.07s\n",
      "       100           1.3064            0.00s\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.3s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3806            3.67s\n",
      "         2           1.3760            3.81s\n",
      "         3           1.3719            3.88s\n",
      "         4           1.3680            3.83s\n",
      "         5           1.3647            3.86s\n",
      "         6           1.3618            3.73s\n",
      "         7           1.3592            3.82s\n",
      "         8           1.3567            3.90s\n",
      "         9           1.3545            3.86s\n",
      "        10           1.3524            3.93s\n",
      "        20           1.3373            3.62s\n",
      "        30           1.3270            3.60s\n",
      "        40           1.3196            3.37s\n",
      "        50           1.3138            3.25s\n",
      "        60           1.3092            3.19s\n",
      "        70           1.3054            3.08s\n",
      "        80           1.3022            2.96s\n",
      "        90           1.2989            2.85s\n",
      "       100           1.2965            2.76s\n",
      "       200           1.2772            2.02s\n",
      "       300           1.2652            1.33s\n",
      "       400           1.2554            0.67s\n",
      "       500           1.2479            0.00s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "roc_scorer = make_scorer(roc_auc_score)\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=5)\n",
    "estimator = GradientBoostingClassifier(random_state=5,verbose=1)\n",
    "\n",
    "major_indices = y_train[y_train==0].index\n",
    "minor_indices = y_train[y_train==1].index\n",
    "sampled_indices = []\n",
    "for i in range(42,52):\n",
    "    sampled_indices.append(resample(y_train[major_indices],replace=False,n_samples=5022,random_state=i).index)\n",
    "    \n",
    "scaler_X = StandardScaler().fit(X_train)\n",
    "#scaler_y = StandardScaler().fit(y_train.to_numpy().reshape(-1,1))\n",
    "scaled_X = scaler_X.transform(X_train)\n",
    "#scaled_y = scaler_y.transform(y_train.to_numpy().reshape(-1,1))\n",
    "scaled_X = pd.DataFrame(scaled_X,index=X_train.index)\n",
    "#scaled_y = pd.Series(scaled_y.flatten(),index=y_train.index)\n",
    "                       \n",
    "params = {'n_estimators':[100,300,500],'max_depth':[2,3],'learning_rate':[0.03,0.06,0.1]}\n",
    "\n",
    "results = []\n",
    "for i in sampled_indices:\n",
    "    X_balanced = pd.concat([scaled_X.loc[i],scaled_X.loc[minor_indices]])\n",
    "    y_balanced = pd.concat([y_train[i],y_train[minor_indices]])\n",
    "    selector = GridSearchCV(estimator=estimator, param_grid=params,cv=kf, scoring=roc_scorer, verbose=1,n_jobs=-1)\n",
    "    selector = selector.fit(X_balanced, y_balanced)\n",
    "    results.append(selector.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1653, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'dict'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 100}    0.598794\n",
       "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 150}    0.601404\n",
       "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100}    0.601681\n",
       "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 200}     0.602077\n",
       "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 100}    0.602793\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}     0.602857\n",
       "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 200}    0.603067\n",
       "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150}     0.603649\n",
       "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 150}    0.603765\n",
       "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 150}    0.604224\n",
       "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100}     0.604229\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}     0.604446\n",
       "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 200}    0.604658\n",
       "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200}    0.604931\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150}     0.604973\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}     0.605144\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}     0.605389\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200}     0.605728\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = results[0]['mean_test_score']\n",
    "for i in range(1,len(results)):\n",
    "    tmp = np.concatenate([tmp,results[i]['mean_test_score']])\n",
    "tmp2 = pd.DataFrame(tmp.reshape(50,18)).apply(lambda x: x.mean(),axis=0)\n",
    "tmp2.index = results[0]['params']\n",
    "tmp2.sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.594209</td>\n",
       "      <td>0.595495</td>\n",
       "      <td>0.595939</td>\n",
       "      <td>0.595939</td>\n",
       "      <td>0.599033</td>\n",
       "      <td>0.597986</td>\n",
       "      <td>0.595652</td>\n",
       "      <td>0.597384</td>\n",
       "      <td>0.602130</td>\n",
       "      <td>0.595786</td>\n",
       "      <td>0.598133</td>\n",
       "      <td>0.598334</td>\n",
       "      <td>0.595405</td>\n",
       "      <td>0.597657</td>\n",
       "      <td>0.599654</td>\n",
       "      <td>0.600377</td>\n",
       "      <td>0.602010</td>\n",
       "      <td>0.601396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.594967</td>\n",
       "      <td>0.597453</td>\n",
       "      <td>0.600203</td>\n",
       "      <td>0.600157</td>\n",
       "      <td>0.601418</td>\n",
       "      <td>0.600467</td>\n",
       "      <td>0.600771</td>\n",
       "      <td>0.602742</td>\n",
       "      <td>0.601426</td>\n",
       "      <td>0.600688</td>\n",
       "      <td>0.600345</td>\n",
       "      <td>0.600881</td>\n",
       "      <td>0.599204</td>\n",
       "      <td>0.598963</td>\n",
       "      <td>0.600337</td>\n",
       "      <td>0.602526</td>\n",
       "      <td>0.598469</td>\n",
       "      <td>0.594979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.601222</td>\n",
       "      <td>0.604661</td>\n",
       "      <td>0.606198</td>\n",
       "      <td>0.606394</td>\n",
       "      <td>0.609098</td>\n",
       "      <td>0.611985</td>\n",
       "      <td>0.606676</td>\n",
       "      <td>0.606658</td>\n",
       "      <td>0.610674</td>\n",
       "      <td>0.605276</td>\n",
       "      <td>0.609146</td>\n",
       "      <td>0.611299</td>\n",
       "      <td>0.608618</td>\n",
       "      <td>0.608060</td>\n",
       "      <td>0.608930</td>\n",
       "      <td>0.608946</td>\n",
       "      <td>0.610145</td>\n",
       "      <td>0.606793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.598003</td>\n",
       "      <td>0.599966</td>\n",
       "      <td>0.602662</td>\n",
       "      <td>0.601090</td>\n",
       "      <td>0.606650</td>\n",
       "      <td>0.607451</td>\n",
       "      <td>0.602243</td>\n",
       "      <td>0.604271</td>\n",
       "      <td>0.607122</td>\n",
       "      <td>0.601049</td>\n",
       "      <td>0.605759</td>\n",
       "      <td>0.607393</td>\n",
       "      <td>0.607892</td>\n",
       "      <td>0.607495</td>\n",
       "      <td>0.605412</td>\n",
       "      <td>0.606464</td>\n",
       "      <td>0.605448</td>\n",
       "      <td>0.604459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.604236</td>\n",
       "      <td>0.605426</td>\n",
       "      <td>0.604474</td>\n",
       "      <td>0.603640</td>\n",
       "      <td>0.606315</td>\n",
       "      <td>0.608026</td>\n",
       "      <td>0.606960</td>\n",
       "      <td>0.607738</td>\n",
       "      <td>0.607708</td>\n",
       "      <td>0.605011</td>\n",
       "      <td>0.608279</td>\n",
       "      <td>0.609440</td>\n",
       "      <td>0.609158</td>\n",
       "      <td>0.608742</td>\n",
       "      <td>0.611313</td>\n",
       "      <td>0.604331</td>\n",
       "      <td>0.605546</td>\n",
       "      <td>0.608234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.600162</td>\n",
       "      <td>0.603219</td>\n",
       "      <td>0.604736</td>\n",
       "      <td>0.603736</td>\n",
       "      <td>0.606054</td>\n",
       "      <td>0.608199</td>\n",
       "      <td>0.607164</td>\n",
       "      <td>0.607301</td>\n",
       "      <td>0.609777</td>\n",
       "      <td>0.603629</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.609895</td>\n",
       "      <td>0.609370</td>\n",
       "      <td>0.610878</td>\n",
       "      <td>0.610712</td>\n",
       "      <td>0.609133</td>\n",
       "      <td>0.606494</td>\n",
       "      <td>0.605534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.599385</td>\n",
       "      <td>0.602155</td>\n",
       "      <td>0.602561</td>\n",
       "      <td>0.600340</td>\n",
       "      <td>0.602609</td>\n",
       "      <td>0.601295</td>\n",
       "      <td>0.602310</td>\n",
       "      <td>0.603843</td>\n",
       "      <td>0.602154</td>\n",
       "      <td>0.602561</td>\n",
       "      <td>0.603819</td>\n",
       "      <td>0.605287</td>\n",
       "      <td>0.603110</td>\n",
       "      <td>0.602820</td>\n",
       "      <td>0.603276</td>\n",
       "      <td>0.602371</td>\n",
       "      <td>0.603393</td>\n",
       "      <td>0.601519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.597859</td>\n",
       "      <td>0.601953</td>\n",
       "      <td>0.602677</td>\n",
       "      <td>0.602887</td>\n",
       "      <td>0.604958</td>\n",
       "      <td>0.606384</td>\n",
       "      <td>0.604729</td>\n",
       "      <td>0.607793</td>\n",
       "      <td>0.611354</td>\n",
       "      <td>0.601176</td>\n",
       "      <td>0.606699</td>\n",
       "      <td>0.608294</td>\n",
       "      <td>0.606212</td>\n",
       "      <td>0.611064</td>\n",
       "      <td>0.611273</td>\n",
       "      <td>0.608527</td>\n",
       "      <td>0.608965</td>\n",
       "      <td>0.609167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.591271</td>\n",
       "      <td>0.594700</td>\n",
       "      <td>0.597932</td>\n",
       "      <td>0.595371</td>\n",
       "      <td>0.597743</td>\n",
       "      <td>0.600510</td>\n",
       "      <td>0.594830</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.596754</td>\n",
       "      <td>0.599461</td>\n",
       "      <td>0.600281</td>\n",
       "      <td>0.598688</td>\n",
       "      <td>0.598957</td>\n",
       "      <td>0.600101</td>\n",
       "      <td>0.598401</td>\n",
       "      <td>0.597057</td>\n",
       "      <td>0.593462</td>\n",
       "      <td>0.589666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.602663</td>\n",
       "      <td>0.606789</td>\n",
       "      <td>0.607856</td>\n",
       "      <td>0.606370</td>\n",
       "      <td>0.612071</td>\n",
       "      <td>0.612861</td>\n",
       "      <td>0.609832</td>\n",
       "      <td>0.611946</td>\n",
       "      <td>0.611926</td>\n",
       "      <td>0.610016</td>\n",
       "      <td>0.612008</td>\n",
       "      <td>0.615694</td>\n",
       "      <td>0.611544</td>\n",
       "      <td>0.612974</td>\n",
       "      <td>0.613342</td>\n",
       "      <td>0.611091</td>\n",
       "      <td>0.611890</td>\n",
       "      <td>0.610228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.603905</td>\n",
       "      <td>0.605781</td>\n",
       "      <td>0.606161</td>\n",
       "      <td>0.606541</td>\n",
       "      <td>0.606295</td>\n",
       "      <td>0.606086</td>\n",
       "      <td>0.603703</td>\n",
       "      <td>0.605573</td>\n",
       "      <td>0.605856</td>\n",
       "      <td>0.606486</td>\n",
       "      <td>0.607797</td>\n",
       "      <td>0.607664</td>\n",
       "      <td>0.604933</td>\n",
       "      <td>0.607708</td>\n",
       "      <td>0.609437</td>\n",
       "      <td>0.602799</td>\n",
       "      <td>0.604103</td>\n",
       "      <td>0.601187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.607182</td>\n",
       "      <td>0.606991</td>\n",
       "      <td>0.607528</td>\n",
       "      <td>0.607411</td>\n",
       "      <td>0.606209</td>\n",
       "      <td>0.606708</td>\n",
       "      <td>0.603900</td>\n",
       "      <td>0.605257</td>\n",
       "      <td>0.606281</td>\n",
       "      <td>0.606828</td>\n",
       "      <td>0.606803</td>\n",
       "      <td>0.606752</td>\n",
       "      <td>0.607261</td>\n",
       "      <td>0.604479</td>\n",
       "      <td>0.604248</td>\n",
       "      <td>0.603775</td>\n",
       "      <td>0.601967</td>\n",
       "      <td>0.600035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.602998</td>\n",
       "      <td>0.604136</td>\n",
       "      <td>0.603166</td>\n",
       "      <td>0.603964</td>\n",
       "      <td>0.604006</td>\n",
       "      <td>0.605307</td>\n",
       "      <td>0.603727</td>\n",
       "      <td>0.602358</td>\n",
       "      <td>0.604967</td>\n",
       "      <td>0.603699</td>\n",
       "      <td>0.605513</td>\n",
       "      <td>0.603712</td>\n",
       "      <td>0.605122</td>\n",
       "      <td>0.606440</td>\n",
       "      <td>0.605381</td>\n",
       "      <td>0.603881</td>\n",
       "      <td>0.604082</td>\n",
       "      <td>0.602503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.588696</td>\n",
       "      <td>0.593998</td>\n",
       "      <td>0.595921</td>\n",
       "      <td>0.595930</td>\n",
       "      <td>0.599751</td>\n",
       "      <td>0.601842</td>\n",
       "      <td>0.597024</td>\n",
       "      <td>0.599528</td>\n",
       "      <td>0.601693</td>\n",
       "      <td>0.596089</td>\n",
       "      <td>0.601272</td>\n",
       "      <td>0.601792</td>\n",
       "      <td>0.600181</td>\n",
       "      <td>0.599997</td>\n",
       "      <td>0.601190</td>\n",
       "      <td>0.599486</td>\n",
       "      <td>0.600855</td>\n",
       "      <td>0.599422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.596531</td>\n",
       "      <td>0.598678</td>\n",
       "      <td>0.602908</td>\n",
       "      <td>0.600908</td>\n",
       "      <td>0.603179</td>\n",
       "      <td>0.606703</td>\n",
       "      <td>0.602359</td>\n",
       "      <td>0.606803</td>\n",
       "      <td>0.604977</td>\n",
       "      <td>0.602274</td>\n",
       "      <td>0.603603</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.603792</td>\n",
       "      <td>0.608874</td>\n",
       "      <td>0.608652</td>\n",
       "      <td>0.604350</td>\n",
       "      <td>0.602114</td>\n",
       "      <td>0.598780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.599195</td>\n",
       "      <td>0.603782</td>\n",
       "      <td>0.605936</td>\n",
       "      <td>0.603249</td>\n",
       "      <td>0.605852</td>\n",
       "      <td>0.605544</td>\n",
       "      <td>0.605517</td>\n",
       "      <td>0.607426</td>\n",
       "      <td>0.607809</td>\n",
       "      <td>0.605690</td>\n",
       "      <td>0.608568</td>\n",
       "      <td>0.609884</td>\n",
       "      <td>0.607005</td>\n",
       "      <td>0.606614</td>\n",
       "      <td>0.606800</td>\n",
       "      <td>0.607994</td>\n",
       "      <td>0.605721</td>\n",
       "      <td>0.603660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.596318</td>\n",
       "      <td>0.598096</td>\n",
       "      <td>0.601407</td>\n",
       "      <td>0.597626</td>\n",
       "      <td>0.601813</td>\n",
       "      <td>0.603087</td>\n",
       "      <td>0.605235</td>\n",
       "      <td>0.603186</td>\n",
       "      <td>0.600597</td>\n",
       "      <td>0.602466</td>\n",
       "      <td>0.602965</td>\n",
       "      <td>0.604361</td>\n",
       "      <td>0.602743</td>\n",
       "      <td>0.604673</td>\n",
       "      <td>0.603586</td>\n",
       "      <td>0.600088</td>\n",
       "      <td>0.599386</td>\n",
       "      <td>0.598940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.597108</td>\n",
       "      <td>0.598645</td>\n",
       "      <td>0.600070</td>\n",
       "      <td>0.598392</td>\n",
       "      <td>0.597687</td>\n",
       "      <td>0.597215</td>\n",
       "      <td>0.599296</td>\n",
       "      <td>0.600837</td>\n",
       "      <td>0.599582</td>\n",
       "      <td>0.599796</td>\n",
       "      <td>0.599422</td>\n",
       "      <td>0.600818</td>\n",
       "      <td>0.597815</td>\n",
       "      <td>0.597256</td>\n",
       "      <td>0.600508</td>\n",
       "      <td>0.599695</td>\n",
       "      <td>0.597541</td>\n",
       "      <td>0.596259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.598899</td>\n",
       "      <td>0.601861</td>\n",
       "      <td>0.604178</td>\n",
       "      <td>0.600906</td>\n",
       "      <td>0.601593</td>\n",
       "      <td>0.605715</td>\n",
       "      <td>0.602019</td>\n",
       "      <td>0.601170</td>\n",
       "      <td>0.602820</td>\n",
       "      <td>0.603587</td>\n",
       "      <td>0.606437</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.603238</td>\n",
       "      <td>0.605822</td>\n",
       "      <td>0.604525</td>\n",
       "      <td>0.605140</td>\n",
       "      <td>0.602159</td>\n",
       "      <td>0.603141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.599754</td>\n",
       "      <td>0.604945</td>\n",
       "      <td>0.604647</td>\n",
       "      <td>0.604749</td>\n",
       "      <td>0.606089</td>\n",
       "      <td>0.605493</td>\n",
       "      <td>0.602779</td>\n",
       "      <td>0.604437</td>\n",
       "      <td>0.603087</td>\n",
       "      <td>0.604124</td>\n",
       "      <td>0.605392</td>\n",
       "      <td>0.605932</td>\n",
       "      <td>0.606438</td>\n",
       "      <td>0.605197</td>\n",
       "      <td>0.607693</td>\n",
       "      <td>0.603509</td>\n",
       "      <td>0.606381</td>\n",
       "      <td>0.602730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.600278</td>\n",
       "      <td>0.601864</td>\n",
       "      <td>0.606498</td>\n",
       "      <td>0.604840</td>\n",
       "      <td>0.607408</td>\n",
       "      <td>0.611105</td>\n",
       "      <td>0.605232</td>\n",
       "      <td>0.608875</td>\n",
       "      <td>0.609635</td>\n",
       "      <td>0.605984</td>\n",
       "      <td>0.608565</td>\n",
       "      <td>0.608917</td>\n",
       "      <td>0.609259</td>\n",
       "      <td>0.610039</td>\n",
       "      <td>0.609386</td>\n",
       "      <td>0.608934</td>\n",
       "      <td>0.608935</td>\n",
       "      <td>0.605530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.598979</td>\n",
       "      <td>0.601208</td>\n",
       "      <td>0.601827</td>\n",
       "      <td>0.603131</td>\n",
       "      <td>0.606831</td>\n",
       "      <td>0.605533</td>\n",
       "      <td>0.607729</td>\n",
       "      <td>0.605054</td>\n",
       "      <td>0.604798</td>\n",
       "      <td>0.603222</td>\n",
       "      <td>0.603516</td>\n",
       "      <td>0.604973</td>\n",
       "      <td>0.607056</td>\n",
       "      <td>0.603415</td>\n",
       "      <td>0.603556</td>\n",
       "      <td>0.606213</td>\n",
       "      <td>0.602174</td>\n",
       "      <td>0.598511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.599795</td>\n",
       "      <td>0.603603</td>\n",
       "      <td>0.605508</td>\n",
       "      <td>0.600051</td>\n",
       "      <td>0.603182</td>\n",
       "      <td>0.604687</td>\n",
       "      <td>0.606033</td>\n",
       "      <td>0.606503</td>\n",
       "      <td>0.606568</td>\n",
       "      <td>0.605251</td>\n",
       "      <td>0.607899</td>\n",
       "      <td>0.608321</td>\n",
       "      <td>0.607177</td>\n",
       "      <td>0.607340</td>\n",
       "      <td>0.606073</td>\n",
       "      <td>0.600971</td>\n",
       "      <td>0.599983</td>\n",
       "      <td>0.599577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.596304</td>\n",
       "      <td>0.600108</td>\n",
       "      <td>0.602464</td>\n",
       "      <td>0.601373</td>\n",
       "      <td>0.600960</td>\n",
       "      <td>0.599919</td>\n",
       "      <td>0.597905</td>\n",
       "      <td>0.596300</td>\n",
       "      <td>0.597587</td>\n",
       "      <td>0.601881</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>0.602648</td>\n",
       "      <td>0.599564</td>\n",
       "      <td>0.599755</td>\n",
       "      <td>0.600334</td>\n",
       "      <td>0.598519</td>\n",
       "      <td>0.597643</td>\n",
       "      <td>0.595925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.599467</td>\n",
       "      <td>0.600877</td>\n",
       "      <td>0.603368</td>\n",
       "      <td>0.602191</td>\n",
       "      <td>0.602913</td>\n",
       "      <td>0.604376</td>\n",
       "      <td>0.601584</td>\n",
       "      <td>0.604708</td>\n",
       "      <td>0.604860</td>\n",
       "      <td>0.602616</td>\n",
       "      <td>0.603713</td>\n",
       "      <td>0.608291</td>\n",
       "      <td>0.602955</td>\n",
       "      <td>0.605992</td>\n",
       "      <td>0.604606</td>\n",
       "      <td>0.605111</td>\n",
       "      <td>0.604748</td>\n",
       "      <td>0.603760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.604125</td>\n",
       "      <td>0.604919</td>\n",
       "      <td>0.605614</td>\n",
       "      <td>0.604680</td>\n",
       "      <td>0.606353</td>\n",
       "      <td>0.606859</td>\n",
       "      <td>0.603147</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.599102</td>\n",
       "      <td>0.604590</td>\n",
       "      <td>0.606003</td>\n",
       "      <td>0.604608</td>\n",
       "      <td>0.606526</td>\n",
       "      <td>0.607420</td>\n",
       "      <td>0.606392</td>\n",
       "      <td>0.604312</td>\n",
       "      <td>0.607040</td>\n",
       "      <td>0.606333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.599501</td>\n",
       "      <td>0.601535</td>\n",
       "      <td>0.602222</td>\n",
       "      <td>0.602597</td>\n",
       "      <td>0.602240</td>\n",
       "      <td>0.601246</td>\n",
       "      <td>0.601360</td>\n",
       "      <td>0.602247</td>\n",
       "      <td>0.601717</td>\n",
       "      <td>0.603035</td>\n",
       "      <td>0.603368</td>\n",
       "      <td>0.601544</td>\n",
       "      <td>0.603166</td>\n",
       "      <td>0.602763</td>\n",
       "      <td>0.602320</td>\n",
       "      <td>0.602921</td>\n",
       "      <td>0.603149</td>\n",
       "      <td>0.600369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.600653</td>\n",
       "      <td>0.601534</td>\n",
       "      <td>0.604381</td>\n",
       "      <td>0.601995</td>\n",
       "      <td>0.599963</td>\n",
       "      <td>0.600684</td>\n",
       "      <td>0.600377</td>\n",
       "      <td>0.604473</td>\n",
       "      <td>0.604654</td>\n",
       "      <td>0.603165</td>\n",
       "      <td>0.604635</td>\n",
       "      <td>0.602324</td>\n",
       "      <td>0.600814</td>\n",
       "      <td>0.601834</td>\n",
       "      <td>0.598059</td>\n",
       "      <td>0.603353</td>\n",
       "      <td>0.603373</td>\n",
       "      <td>0.598738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.608690</td>\n",
       "      <td>0.606441</td>\n",
       "      <td>0.607989</td>\n",
       "      <td>0.607898</td>\n",
       "      <td>0.608687</td>\n",
       "      <td>0.611154</td>\n",
       "      <td>0.608556</td>\n",
       "      <td>0.607380</td>\n",
       "      <td>0.611030</td>\n",
       "      <td>0.607568</td>\n",
       "      <td>0.610425</td>\n",
       "      <td>0.611471</td>\n",
       "      <td>0.609053</td>\n",
       "      <td>0.610136</td>\n",
       "      <td>0.611019</td>\n",
       "      <td>0.610419</td>\n",
       "      <td>0.609755</td>\n",
       "      <td>0.609317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.599582</td>\n",
       "      <td>0.602114</td>\n",
       "      <td>0.603582</td>\n",
       "      <td>0.602884</td>\n",
       "      <td>0.604387</td>\n",
       "      <td>0.609239</td>\n",
       "      <td>0.602534</td>\n",
       "      <td>0.606045</td>\n",
       "      <td>0.604824</td>\n",
       "      <td>0.603707</td>\n",
       "      <td>0.607417</td>\n",
       "      <td>0.606916</td>\n",
       "      <td>0.606005</td>\n",
       "      <td>0.607072</td>\n",
       "      <td>0.605958</td>\n",
       "      <td>0.606206</td>\n",
       "      <td>0.604070</td>\n",
       "      <td>0.599347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.591254</td>\n",
       "      <td>0.594354</td>\n",
       "      <td>0.596941</td>\n",
       "      <td>0.596305</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.597583</td>\n",
       "      <td>0.595145</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.596578</td>\n",
       "      <td>0.596965</td>\n",
       "      <td>0.598917</td>\n",
       "      <td>0.598258</td>\n",
       "      <td>0.599181</td>\n",
       "      <td>0.600054</td>\n",
       "      <td>0.602683</td>\n",
       "      <td>0.596336</td>\n",
       "      <td>0.595174</td>\n",
       "      <td>0.596773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.593768</td>\n",
       "      <td>0.596770</td>\n",
       "      <td>0.597806</td>\n",
       "      <td>0.597733</td>\n",
       "      <td>0.599179</td>\n",
       "      <td>0.599948</td>\n",
       "      <td>0.598269</td>\n",
       "      <td>0.598189</td>\n",
       "      <td>0.595341</td>\n",
       "      <td>0.598724</td>\n",
       "      <td>0.600890</td>\n",
       "      <td>0.598973</td>\n",
       "      <td>0.599722</td>\n",
       "      <td>0.597776</td>\n",
       "      <td>0.596063</td>\n",
       "      <td>0.593338</td>\n",
       "      <td>0.598716</td>\n",
       "      <td>0.595659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.599340</td>\n",
       "      <td>0.604353</td>\n",
       "      <td>0.606099</td>\n",
       "      <td>0.604803</td>\n",
       "      <td>0.607780</td>\n",
       "      <td>0.612162</td>\n",
       "      <td>0.606869</td>\n",
       "      <td>0.609051</td>\n",
       "      <td>0.610582</td>\n",
       "      <td>0.605693</td>\n",
       "      <td>0.607756</td>\n",
       "      <td>0.609628</td>\n",
       "      <td>0.610676</td>\n",
       "      <td>0.614191</td>\n",
       "      <td>0.611702</td>\n",
       "      <td>0.613324</td>\n",
       "      <td>0.609570</td>\n",
       "      <td>0.611294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.597389</td>\n",
       "      <td>0.601524</td>\n",
       "      <td>0.607235</td>\n",
       "      <td>0.603021</td>\n",
       "      <td>0.607813</td>\n",
       "      <td>0.611198</td>\n",
       "      <td>0.605504</td>\n",
       "      <td>0.607909</td>\n",
       "      <td>0.606723</td>\n",
       "      <td>0.606533</td>\n",
       "      <td>0.607725</td>\n",
       "      <td>0.608174</td>\n",
       "      <td>0.608751</td>\n",
       "      <td>0.608503</td>\n",
       "      <td>0.608020</td>\n",
       "      <td>0.608171</td>\n",
       "      <td>0.607047</td>\n",
       "      <td>0.605882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.602430</td>\n",
       "      <td>0.608103</td>\n",
       "      <td>0.609537</td>\n",
       "      <td>0.608704</td>\n",
       "      <td>0.611408</td>\n",
       "      <td>0.612421</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>0.613291</td>\n",
       "      <td>0.613201</td>\n",
       "      <td>0.610289</td>\n",
       "      <td>0.612890</td>\n",
       "      <td>0.613767</td>\n",
       "      <td>0.613428</td>\n",
       "      <td>0.615668</td>\n",
       "      <td>0.612546</td>\n",
       "      <td>0.613782</td>\n",
       "      <td>0.611953</td>\n",
       "      <td>0.612434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.599800</td>\n",
       "      <td>0.602903</td>\n",
       "      <td>0.602721</td>\n",
       "      <td>0.603695</td>\n",
       "      <td>0.605381</td>\n",
       "      <td>0.606199</td>\n",
       "      <td>0.604186</td>\n",
       "      <td>0.606983</td>\n",
       "      <td>0.607608</td>\n",
       "      <td>0.600833</td>\n",
       "      <td>0.604892</td>\n",
       "      <td>0.604608</td>\n",
       "      <td>0.607293</td>\n",
       "      <td>0.608131</td>\n",
       "      <td>0.610701</td>\n",
       "      <td>0.608002</td>\n",
       "      <td>0.607021</td>\n",
       "      <td>0.604916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.597456</td>\n",
       "      <td>0.599649</td>\n",
       "      <td>0.603670</td>\n",
       "      <td>0.599715</td>\n",
       "      <td>0.602690</td>\n",
       "      <td>0.602042</td>\n",
       "      <td>0.599404</td>\n",
       "      <td>0.602329</td>\n",
       "      <td>0.601024</td>\n",
       "      <td>0.601605</td>\n",
       "      <td>0.603792</td>\n",
       "      <td>0.604497</td>\n",
       "      <td>0.600437</td>\n",
       "      <td>0.603507</td>\n",
       "      <td>0.605517</td>\n",
       "      <td>0.599663</td>\n",
       "      <td>0.601313</td>\n",
       "      <td>0.604328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.596376</td>\n",
       "      <td>0.599650</td>\n",
       "      <td>0.603251</td>\n",
       "      <td>0.602235</td>\n",
       "      <td>0.603184</td>\n",
       "      <td>0.605132</td>\n",
       "      <td>0.600551</td>\n",
       "      <td>0.604855</td>\n",
       "      <td>0.604693</td>\n",
       "      <td>0.600890</td>\n",
       "      <td>0.608086</td>\n",
       "      <td>0.607488</td>\n",
       "      <td>0.605055</td>\n",
       "      <td>0.606780</td>\n",
       "      <td>0.604898</td>\n",
       "      <td>0.603372</td>\n",
       "      <td>0.600976</td>\n",
       "      <td>0.601365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.595208</td>\n",
       "      <td>0.600707</td>\n",
       "      <td>0.603466</td>\n",
       "      <td>0.598272</td>\n",
       "      <td>0.603151</td>\n",
       "      <td>0.603947</td>\n",
       "      <td>0.603968</td>\n",
       "      <td>0.604479</td>\n",
       "      <td>0.604179</td>\n",
       "      <td>0.606390</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.609620</td>\n",
       "      <td>0.605952</td>\n",
       "      <td>0.604789</td>\n",
       "      <td>0.604508</td>\n",
       "      <td>0.602287</td>\n",
       "      <td>0.604530</td>\n",
       "      <td>0.603610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.600655</td>\n",
       "      <td>0.601565</td>\n",
       "      <td>0.597934</td>\n",
       "      <td>0.600194</td>\n",
       "      <td>0.598821</td>\n",
       "      <td>0.598744</td>\n",
       "      <td>0.596679</td>\n",
       "      <td>0.599824</td>\n",
       "      <td>0.598739</td>\n",
       "      <td>0.597919</td>\n",
       "      <td>0.599635</td>\n",
       "      <td>0.600353</td>\n",
       "      <td>0.598041</td>\n",
       "      <td>0.599204</td>\n",
       "      <td>0.601223</td>\n",
       "      <td>0.599535</td>\n",
       "      <td>0.603440</td>\n",
       "      <td>0.602263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.598676</td>\n",
       "      <td>0.601622</td>\n",
       "      <td>0.603637</td>\n",
       "      <td>0.601503</td>\n",
       "      <td>0.605171</td>\n",
       "      <td>0.606615</td>\n",
       "      <td>0.604439</td>\n",
       "      <td>0.608050</td>\n",
       "      <td>0.608209</td>\n",
       "      <td>0.603762</td>\n",
       "      <td>0.606042</td>\n",
       "      <td>0.608150</td>\n",
       "      <td>0.608090</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.608213</td>\n",
       "      <td>0.605850</td>\n",
       "      <td>0.604026</td>\n",
       "      <td>0.604718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.602055</td>\n",
       "      <td>0.604208</td>\n",
       "      <td>0.599844</td>\n",
       "      <td>0.604273</td>\n",
       "      <td>0.607356</td>\n",
       "      <td>0.603092</td>\n",
       "      <td>0.605522</td>\n",
       "      <td>0.608656</td>\n",
       "      <td>0.604794</td>\n",
       "      <td>0.606964</td>\n",
       "      <td>0.609931</td>\n",
       "      <td>0.607954</td>\n",
       "      <td>0.610522</td>\n",
       "      <td>0.607527</td>\n",
       "      <td>0.611425</td>\n",
       "      <td>0.609256</td>\n",
       "      <td>0.606021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.597004</td>\n",
       "      <td>0.601297</td>\n",
       "      <td>0.603917</td>\n",
       "      <td>0.602926</td>\n",
       "      <td>0.606338</td>\n",
       "      <td>0.608037</td>\n",
       "      <td>0.605222</td>\n",
       "      <td>0.605303</td>\n",
       "      <td>0.608526</td>\n",
       "      <td>0.604379</td>\n",
       "      <td>0.607762</td>\n",
       "      <td>0.608800</td>\n",
       "      <td>0.606972</td>\n",
       "      <td>0.605646</td>\n",
       "      <td>0.605646</td>\n",
       "      <td>0.607925</td>\n",
       "      <td>0.605485</td>\n",
       "      <td>0.599761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.596732</td>\n",
       "      <td>0.598534</td>\n",
       "      <td>0.598866</td>\n",
       "      <td>0.600981</td>\n",
       "      <td>0.603135</td>\n",
       "      <td>0.602748</td>\n",
       "      <td>0.602439</td>\n",
       "      <td>0.602988</td>\n",
       "      <td>0.603912</td>\n",
       "      <td>0.598986</td>\n",
       "      <td>0.601464</td>\n",
       "      <td>0.604184</td>\n",
       "      <td>0.600545</td>\n",
       "      <td>0.605564</td>\n",
       "      <td>0.601563</td>\n",
       "      <td>0.602269</td>\n",
       "      <td>0.601526</td>\n",
       "      <td>0.600990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.598974</td>\n",
       "      <td>0.601011</td>\n",
       "      <td>0.602731</td>\n",
       "      <td>0.599141</td>\n",
       "      <td>0.603614</td>\n",
       "      <td>0.606328</td>\n",
       "      <td>0.600907</td>\n",
       "      <td>0.605108</td>\n",
       "      <td>0.605773</td>\n",
       "      <td>0.600475</td>\n",
       "      <td>0.604197</td>\n",
       "      <td>0.604941</td>\n",
       "      <td>0.602609</td>\n",
       "      <td>0.605449</td>\n",
       "      <td>0.604794</td>\n",
       "      <td>0.604515</td>\n",
       "      <td>0.605692</td>\n",
       "      <td>0.604315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.595147</td>\n",
       "      <td>0.598834</td>\n",
       "      <td>0.599476</td>\n",
       "      <td>0.597659</td>\n",
       "      <td>0.599245</td>\n",
       "      <td>0.601322</td>\n",
       "      <td>0.597773</td>\n",
       "      <td>0.599380</td>\n",
       "      <td>0.600725</td>\n",
       "      <td>0.598693</td>\n",
       "      <td>0.598832</td>\n",
       "      <td>0.601530</td>\n",
       "      <td>0.601581</td>\n",
       "      <td>0.602968</td>\n",
       "      <td>0.602036</td>\n",
       "      <td>0.601590</td>\n",
       "      <td>0.599369</td>\n",
       "      <td>0.594602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.603695</td>\n",
       "      <td>0.606048</td>\n",
       "      <td>0.606330</td>\n",
       "      <td>0.603715</td>\n",
       "      <td>0.607147</td>\n",
       "      <td>0.607394</td>\n",
       "      <td>0.605892</td>\n",
       "      <td>0.606191</td>\n",
       "      <td>0.603854</td>\n",
       "      <td>0.606611</td>\n",
       "      <td>0.608445</td>\n",
       "      <td>0.605672</td>\n",
       "      <td>0.603777</td>\n",
       "      <td>0.604983</td>\n",
       "      <td>0.606757</td>\n",
       "      <td>0.606467</td>\n",
       "      <td>0.604398</td>\n",
       "      <td>0.601213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.594103</td>\n",
       "      <td>0.597034</td>\n",
       "      <td>0.597578</td>\n",
       "      <td>0.595538</td>\n",
       "      <td>0.598022</td>\n",
       "      <td>0.599108</td>\n",
       "      <td>0.600795</td>\n",
       "      <td>0.602226</td>\n",
       "      <td>0.602448</td>\n",
       "      <td>0.597179</td>\n",
       "      <td>0.599650</td>\n",
       "      <td>0.600314</td>\n",
       "      <td>0.597689</td>\n",
       "      <td>0.596453</td>\n",
       "      <td>0.597710</td>\n",
       "      <td>0.599363</td>\n",
       "      <td>0.594658</td>\n",
       "      <td>0.594321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.598902</td>\n",
       "      <td>0.601594</td>\n",
       "      <td>0.606210</td>\n",
       "      <td>0.600121</td>\n",
       "      <td>0.599871</td>\n",
       "      <td>0.599933</td>\n",
       "      <td>0.601395</td>\n",
       "      <td>0.601646</td>\n",
       "      <td>0.602028</td>\n",
       "      <td>0.604117</td>\n",
       "      <td>0.604403</td>\n",
       "      <td>0.603915</td>\n",
       "      <td>0.601792</td>\n",
       "      <td>0.601711</td>\n",
       "      <td>0.599859</td>\n",
       "      <td>0.602755</td>\n",
       "      <td>0.604055</td>\n",
       "      <td>0.602616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.603206</td>\n",
       "      <td>0.599656</td>\n",
       "      <td>0.601119</td>\n",
       "      <td>0.600703</td>\n",
       "      <td>0.602708</td>\n",
       "      <td>0.604683</td>\n",
       "      <td>0.604158</td>\n",
       "      <td>0.604845</td>\n",
       "      <td>0.606634</td>\n",
       "      <td>0.601280</td>\n",
       "      <td>0.601505</td>\n",
       "      <td>0.603227</td>\n",
       "      <td>0.603206</td>\n",
       "      <td>0.605099</td>\n",
       "      <td>0.602861</td>\n",
       "      <td>0.602977</td>\n",
       "      <td>0.603259</td>\n",
       "      <td>0.600713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.594209  0.595495  0.595939  0.595939  0.599033  0.597986  0.595652   \n",
       "1   0.594967  0.597453  0.600203  0.600157  0.601418  0.600467  0.600771   \n",
       "2   0.601222  0.604661  0.606198  0.606394  0.609098  0.611985  0.606676   \n",
       "3   0.598003  0.599966  0.602662  0.601090  0.606650  0.607451  0.602243   \n",
       "4   0.604236  0.605426  0.604474  0.603640  0.606315  0.608026  0.606960   \n",
       "5   0.600162  0.603219  0.604736  0.603736  0.606054  0.608199  0.607164   \n",
       "6   0.599385  0.602155  0.602561  0.600340  0.602609  0.601295  0.602310   \n",
       "7   0.597859  0.601953  0.602677  0.602887  0.604958  0.606384  0.604729   \n",
       "8   0.591271  0.594700  0.597932  0.595371  0.597743  0.600510  0.594830   \n",
       "9   0.602663  0.606789  0.607856  0.606370  0.612071  0.612861  0.609832   \n",
       "10  0.603905  0.605781  0.606161  0.606541  0.606295  0.606086  0.603703   \n",
       "11  0.607182  0.606991  0.607528  0.607411  0.606209  0.606708  0.603900   \n",
       "12  0.602998  0.604136  0.603166  0.603964  0.604006  0.605307  0.603727   \n",
       "13  0.588696  0.593998  0.595921  0.595930  0.599751  0.601842  0.597024   \n",
       "14  0.596531  0.598678  0.602908  0.600908  0.603179  0.606703  0.602359   \n",
       "15  0.599195  0.603782  0.605936  0.603249  0.605852  0.605544  0.605517   \n",
       "16  0.596318  0.598096  0.601407  0.597626  0.601813  0.603087  0.605235   \n",
       "17  0.597108  0.598645  0.600070  0.598392  0.597687  0.597215  0.599296   \n",
       "18  0.598899  0.601861  0.604178  0.600906  0.601593  0.605715  0.602019   \n",
       "19  0.599754  0.604945  0.604647  0.604749  0.606089  0.605493  0.602779   \n",
       "20  0.600278  0.601864  0.606498  0.604840  0.607408  0.611105  0.605232   \n",
       "21  0.598979  0.601208  0.601827  0.603131  0.606831  0.605533  0.607729   \n",
       "22  0.599795  0.603603  0.605508  0.600051  0.603182  0.604687  0.606033   \n",
       "23  0.596304  0.600108  0.602464  0.601373  0.600960  0.599919  0.597905   \n",
       "24  0.599467  0.600877  0.603368  0.602191  0.602913  0.604376  0.601584   \n",
       "25  0.604125  0.604919  0.605614  0.604680  0.606353  0.606859  0.603147   \n",
       "26  0.599501  0.601535  0.602222  0.602597  0.602240  0.601246  0.601360   \n",
       "27  0.600653  0.601534  0.604381  0.601995  0.599963  0.600684  0.600377   \n",
       "28  0.608690  0.606441  0.607989  0.607898  0.608687  0.611154  0.608556   \n",
       "29  0.599582  0.602114  0.603582  0.602884  0.604387  0.609239  0.602534   \n",
       "30  0.591254  0.594354  0.596941  0.596305  0.597981  0.597583  0.595145   \n",
       "31  0.593768  0.596770  0.597806  0.597733  0.599179  0.599948  0.598269   \n",
       "32  0.599340  0.604353  0.606099  0.604803  0.607780  0.612162  0.606869   \n",
       "33  0.597389  0.601524  0.607235  0.603021  0.607813  0.611198  0.605504   \n",
       "34  0.602430  0.608103  0.609537  0.608704  0.611408  0.612421  0.611765   \n",
       "35  0.599800  0.602903  0.602721  0.603695  0.605381  0.606199  0.604186   \n",
       "36  0.597456  0.599649  0.603670  0.599715  0.602690  0.602042  0.599404   \n",
       "37  0.596376  0.599650  0.603251  0.602235  0.603184  0.605132  0.600551   \n",
       "38  0.595208  0.600707  0.603466  0.598272  0.603151  0.603947  0.603968   \n",
       "39  0.600655  0.601565  0.597934  0.600194  0.598821  0.598744  0.596679   \n",
       "40  0.598676  0.601622  0.603637  0.601503  0.605171  0.606615  0.604439   \n",
       "41  0.597634  0.602055  0.604208  0.599844  0.604273  0.607356  0.603092   \n",
       "42  0.597004  0.601297  0.603917  0.602926  0.606338  0.608037  0.605222   \n",
       "43  0.596732  0.598534  0.598866  0.600981  0.603135  0.602748  0.602439   \n",
       "44  0.598974  0.601011  0.602731  0.599141  0.603614  0.606328  0.600907   \n",
       "45  0.595147  0.598834  0.599476  0.597659  0.599245  0.601322  0.597773   \n",
       "46  0.603695  0.606048  0.606330  0.603715  0.607147  0.607394  0.605892   \n",
       "47  0.594103  0.597034  0.597578  0.595538  0.598022  0.599108  0.600795   \n",
       "48  0.598902  0.601594  0.606210  0.600121  0.599871  0.599933  0.601395   \n",
       "49  0.603206  0.599656  0.601119  0.600703  0.602708  0.604683  0.604158   \n",
       "\n",
       "           7         8         9        10        11        12        13  \\\n",
       "0   0.597384  0.602130  0.595786  0.598133  0.598334  0.595405  0.597657   \n",
       "1   0.602742  0.601426  0.600688  0.600345  0.600881  0.599204  0.598963   \n",
       "2   0.606658  0.610674  0.605276  0.609146  0.611299  0.608618  0.608060   \n",
       "3   0.604271  0.607122  0.601049  0.605759  0.607393  0.607892  0.607495   \n",
       "4   0.607738  0.607708  0.605011  0.608279  0.609440  0.609158  0.608742   \n",
       "5   0.607301  0.609777  0.603629  0.606061  0.609895  0.609370  0.610878   \n",
       "6   0.603843  0.602154  0.602561  0.603819  0.605287  0.603110  0.602820   \n",
       "7   0.607793  0.611354  0.601176  0.606699  0.608294  0.606212  0.611064   \n",
       "8   0.596100  0.596754  0.599461  0.600281  0.598688  0.598957  0.600101   \n",
       "9   0.611946  0.611926  0.610016  0.612008  0.615694  0.611544  0.612974   \n",
       "10  0.605573  0.605856  0.606486  0.607797  0.607664  0.604933  0.607708   \n",
       "11  0.605257  0.606281  0.606828  0.606803  0.606752  0.607261  0.604479   \n",
       "12  0.602358  0.604967  0.603699  0.605513  0.603712  0.605122  0.606440   \n",
       "13  0.599528  0.601693  0.596089  0.601272  0.601792  0.600181  0.599997   \n",
       "14  0.606803  0.604977  0.602274  0.603603  0.606367  0.603792  0.608874   \n",
       "15  0.607426  0.607809  0.605690  0.608568  0.609884  0.607005  0.606614   \n",
       "16  0.603186  0.600597  0.602466  0.602965  0.604361  0.602743  0.604673   \n",
       "17  0.600837  0.599582  0.599796  0.599422  0.600818  0.597815  0.597256   \n",
       "18  0.601170  0.602820  0.603587  0.606437  0.607850  0.603238  0.605822   \n",
       "19  0.604437  0.603087  0.604124  0.605392  0.605932  0.606438  0.605197   \n",
       "20  0.608875  0.609635  0.605984  0.608565  0.608917  0.609259  0.610039   \n",
       "21  0.605054  0.604798  0.603222  0.603516  0.604973  0.607056  0.603415   \n",
       "22  0.606503  0.606568  0.605251  0.607899  0.608321  0.607177  0.607340   \n",
       "23  0.596300  0.597587  0.601881  0.603264  0.602648  0.599564  0.599755   \n",
       "24  0.604708  0.604860  0.602616  0.603713  0.608291  0.602955  0.605992   \n",
       "25  0.604167  0.599102  0.604590  0.606003  0.604608  0.606526  0.607420   \n",
       "26  0.602247  0.601717  0.603035  0.603368  0.601544  0.603166  0.602763   \n",
       "27  0.604473  0.604654  0.603165  0.604635  0.602324  0.600814  0.601834   \n",
       "28  0.607380  0.611030  0.607568  0.610425  0.611471  0.609053  0.610136   \n",
       "29  0.606045  0.604824  0.603707  0.607417  0.606916  0.606005  0.607072   \n",
       "30  0.594937  0.596578  0.596965  0.598917  0.598258  0.599181  0.600054   \n",
       "31  0.598189  0.595341  0.598724  0.600890  0.598973  0.599722  0.597776   \n",
       "32  0.609051  0.610582  0.605693  0.607756  0.609628  0.610676  0.614191   \n",
       "33  0.607909  0.606723  0.606533  0.607725  0.608174  0.608751  0.608503   \n",
       "34  0.613291  0.613201  0.610289  0.612890  0.613767  0.613428  0.615668   \n",
       "35  0.606983  0.607608  0.600833  0.604892  0.604608  0.607293  0.608131   \n",
       "36  0.602329  0.601024  0.601605  0.603792  0.604497  0.600437  0.603507   \n",
       "37  0.604855  0.604693  0.600890  0.608086  0.607488  0.605055  0.606780   \n",
       "38  0.604479  0.604179  0.606390  0.607717  0.609620  0.605952  0.604789   \n",
       "39  0.599824  0.598739  0.597919  0.599635  0.600353  0.598041  0.599204   \n",
       "40  0.608050  0.608209  0.603762  0.606042  0.608150  0.608090  0.610900   \n",
       "41  0.605522  0.608656  0.604794  0.606964  0.609931  0.607954  0.610522   \n",
       "42  0.605303  0.608526  0.604379  0.607762  0.608800  0.606972  0.605646   \n",
       "43  0.602988  0.603912  0.598986  0.601464  0.604184  0.600545  0.605564   \n",
       "44  0.605108  0.605773  0.600475  0.604197  0.604941  0.602609  0.605449   \n",
       "45  0.599380  0.600725  0.598693  0.598832  0.601530  0.601581  0.602968   \n",
       "46  0.606191  0.603854  0.606611  0.608445  0.605672  0.603777  0.604983   \n",
       "47  0.602226  0.602448  0.597179  0.599650  0.600314  0.597689  0.596453   \n",
       "48  0.601646  0.602028  0.604117  0.604403  0.603915  0.601792  0.601711   \n",
       "49  0.604845  0.606634  0.601280  0.601505  0.603227  0.603206  0.605099   \n",
       "\n",
       "          14        15        16        17  \n",
       "0   0.599654  0.600377  0.602010  0.601396  \n",
       "1   0.600337  0.602526  0.598469  0.594979  \n",
       "2   0.608930  0.608946  0.610145  0.606793  \n",
       "3   0.605412  0.606464  0.605448  0.604459  \n",
       "4   0.611313  0.604331  0.605546  0.608234  \n",
       "5   0.610712  0.609133  0.606494  0.605534  \n",
       "6   0.603276  0.602371  0.603393  0.601519  \n",
       "7   0.611273  0.608527  0.608965  0.609167  \n",
       "8   0.598401  0.597057  0.593462  0.589666  \n",
       "9   0.613342  0.611091  0.611890  0.610228  \n",
       "10  0.609437  0.602799  0.604103  0.601187  \n",
       "11  0.604248  0.603775  0.601967  0.600035  \n",
       "12  0.605381  0.603881  0.604082  0.602503  \n",
       "13  0.601190  0.599486  0.600855  0.599422  \n",
       "14  0.608652  0.604350  0.602114  0.598780  \n",
       "15  0.606800  0.607994  0.605721  0.603660  \n",
       "16  0.603586  0.600088  0.599386  0.598940  \n",
       "17  0.600508  0.599695  0.597541  0.596259  \n",
       "18  0.604525  0.605140  0.602159  0.603141  \n",
       "19  0.607693  0.603509  0.606381  0.602730  \n",
       "20  0.609386  0.608934  0.608935  0.605530  \n",
       "21  0.603556  0.606213  0.602174  0.598511  \n",
       "22  0.606073  0.600971  0.599983  0.599577  \n",
       "23  0.600334  0.598519  0.597643  0.595925  \n",
       "24  0.604606  0.605111  0.604748  0.603760  \n",
       "25  0.606392  0.604312  0.607040  0.606333  \n",
       "26  0.602320  0.602921  0.603149  0.600369  \n",
       "27  0.598059  0.603353  0.603373  0.598738  \n",
       "28  0.611019  0.610419  0.609755  0.609317  \n",
       "29  0.605958  0.606206  0.604070  0.599347  \n",
       "30  0.602683  0.596336  0.595174  0.596773  \n",
       "31  0.596063  0.593338  0.598716  0.595659  \n",
       "32  0.611702  0.613324  0.609570  0.611294  \n",
       "33  0.608020  0.608171  0.607047  0.605882  \n",
       "34  0.612546  0.613782  0.611953  0.612434  \n",
       "35  0.610701  0.608002  0.607021  0.604916  \n",
       "36  0.605517  0.599663  0.601313  0.604328  \n",
       "37  0.604898  0.603372  0.600976  0.601365  \n",
       "38  0.604508  0.602287  0.604530  0.603610  \n",
       "39  0.601223  0.599535  0.603440  0.602263  \n",
       "40  0.608213  0.605850  0.604026  0.604718  \n",
       "41  0.607527  0.611425  0.609256  0.606021  \n",
       "42  0.605646  0.607925  0.605485  0.599761  \n",
       "43  0.601563  0.602269  0.601526  0.600990  \n",
       "44  0.604794  0.604515  0.605692  0.604315  \n",
       "45  0.602036  0.601590  0.599369  0.594602  \n",
       "46  0.606757  0.606467  0.604398  0.601213  \n",
       "47  0.597710  0.599363  0.594658  0.594321  \n",
       "48  0.599859  0.602755  0.604055  0.602616  \n",
       "49  0.602861  0.602977  0.603259  0.600713  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tmp.reshape(50,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1653, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'dict'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 50}     0.003016\n",
       "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 200}    0.003659\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}      0.003716\n",
       "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50}     0.003750\n",
       "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100}    0.003813\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}     0.003847\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}      0.003986\n",
       "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 100}    0.004012\n",
       "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 150}    0.004177\n",
       "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 250}    0.004243\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150}     0.004398\n",
       "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 150}    0.004441\n",
       "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100}     0.004500\n",
       "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300}    0.004748\n",
       "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 50}     0.004802\n",
       "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 150}    0.004876\n",
       "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 100}    0.004976\n",
       "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50}      0.005045\n",
       "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 350}    0.005051\n",
       "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 300}    0.005096\n",
       "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200}    0.005152\n",
       "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 200}    0.005165\n",
       "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 250}    0.005287\n",
       "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 250}    0.005309\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}     0.005455\n",
       "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 350}     0.005534\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}     0.005537\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}     0.005556\n",
       "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150}     0.005557\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 250}     0.005561\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300}     0.005601\n",
       "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 350}    0.005631\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200}     0.005701\n",
       "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300}    0.005927\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 350}     0.005967\n",
       "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300}     0.006012\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}     0.006028\n",
       "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 350}    0.006207\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}     0.006409\n",
       "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 250}     0.006480\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 350}     0.006483\n",
       "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 200}     0.006558\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = results[0]['mean_test_score']\n",
    "for i in range(1,len(results)):\n",
    "    tmp = np.concatenate([tmp,results[i]['mean_test_score']])\n",
    "tmp2 = pd.DataFrame(tmp.reshape(10,42)).apply(lambda x: x.std(),axis=0)\n",
    "tmp2.index = results[0]['params']\n",
    "tmp2.sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1653, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'dict'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 100}    0.595196\n",
       "{'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 100}    0.598567\n",
       "{'learning_rate': 0.06, 'max_depth': 2, 'n_estimators': 100}    0.600123\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}     0.600404\n",
       "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 300}    0.601977\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}     0.602465\n",
       "{'learning_rate': 0.06, 'max_depth': 3, 'n_estimators': 100}    0.602970\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}     0.604256\n",
       "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 500}    0.604605\n",
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}     0.604947\n",
       "{'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 300}    0.605303\n",
       "{'learning_rate': 0.06, 'max_depth': 2, 'n_estimators': 300}    0.605482\n",
       "{'learning_rate': 0.06, 'max_depth': 3, 'n_estimators': 500}    0.605728\n",
       "{'learning_rate': 0.06, 'max_depth': 3, 'n_estimators': 300}    0.606188\n",
       "{'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 500}    0.606263\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500}     0.606640\n",
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300}     0.607159\n",
       "{'learning_rate': 0.06, 'max_depth': 2, 'n_estimators': 500}    0.607229\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = results[0]['mean_test_score']\n",
    "for i in range(1,len(results)):\n",
    "    tmp = np.concatenate([tmp,results[i]['mean_test_score']])\n",
    "tmp2 = pd.DataFrame(tmp.reshape(10,18)).apply(lambda x: x.mean(),axis=0)\n",
    "tmp2.index = results[0]['params']\n",
    "tmp2.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 768 candidates, totalling 7680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed: 44.2min\n",
      "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed: 109.4min\n",
      "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed: 186.6min\n",
      "[Parallel(n_jobs=2)]: Done 1796 tasks      | elapsed: 276.7min\n",
      "[Parallel(n_jobs=2)]: Done 2446 tasks      | elapsed: 379.7min\n",
      "[Parallel(n_jobs=2)]: Done 3196 tasks      | elapsed: 510.0min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-c0b858db28de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroc_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_balanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Upsampling\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "roc_scorer = make_scorer(roc_auc_score)\n",
    "kf = KFold(n_splits=10,shuffle=True,random_state=5)\n",
    "estimator = GradientBoostingClassifier(random_state=5,verbose=1)\n",
    "\n",
    "major_indices = y_train[y_train==0].index\n",
    "minor_indices = y_train[y_train==1].index\n",
    "sampled_indices = resample(y_train[minor_indices],replace=True,n_samples=50954,random_state=5).index\n",
    "X_balanced = pd.concat([X_train.loc[major_indices],X_train.loc[sampled_indices]])\n",
    "y_balanced = pd.concat([y_train[major_indices],y_train[sampled_indices]])\n",
    "\n",
    "scaler_X = StandardScaler().fit(X_balanced)\n",
    "#scaler_y = StandardScaler().fit(y_train.to_numpy().reshape(-1,1))\n",
    "scaled_X = scaler_X.transform(X_balanced)\n",
    "#scaled_y = scaler_y.transform(y_train.to_numpy().reshape(-1,1))\n",
    "scaled_X = pd.DataFrame(scaled_X,index=X_balanced.index)\n",
    "#scaled_y = pd.Series(scaled_y.flatten(),index=y_train.index)\n",
    "                       \n",
    "params = {'n_estimators':[50,100,150,200,250,300,400,500],'max_depth':[2,3,4,5],'learning_rate':[0.03,0.06,0.1,.13],\n",
    "         'max_features':[0.1,0.3,0.5],'loss':['deviance','exponential']}\n",
    "\n",
    "selector = GridSearchCV(estimator=estimator, param_grid=params,cv=kf,scoring=roc_scorer,verbose=1,n_jobs=2)\n",
    "selector = selector.fit(scaled_X, y_balanced)\n",
    "print(selector.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_check_is_fitted',\n",
       " '_estimator_type',\n",
       " '_format_results',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_required_parameters',\n",
       " '_run_search',\n",
       " 'classes_',\n",
       " 'cv',\n",
       " 'decision_function',\n",
       " 'error_score',\n",
       " 'estimator',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'iid',\n",
       " 'inverse_transform',\n",
       " 'multimetric_',\n",
       " 'n_jobs',\n",
       " 'param_grid',\n",
       " 'pre_dispatch',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'refit',\n",
       " 'return_train_score',\n",
       " 'score',\n",
       " 'scoring',\n",
       " 'set_params',\n",
       " 'transform',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 300},\n",
       " {'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 500},\n",
       " {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 300},\n",
       " {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 500},\n",
       " {'learning_rate': 0.06, 'max_depth': 2, 'n_estimators': 300},\n",
       " {'learning_rate': 0.06, 'max_depth': 2, 'n_estimators': 500},\n",
       " {'learning_rate': 0.06, 'max_depth': 3, 'n_estimators': 300},\n",
       " {'learning_rate': 0.06, 'max_depth': 3, 'n_estimators': 500},\n",
       " {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300},\n",
       " {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500},\n",
       " {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300},\n",
       " {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60956997, 0.61540718, 0.62099921, 0.62817333, 0.61802478,\n",
       "       0.62051535, 0.63165802, 0.64096884, 0.62168573, 0.62852825,\n",
       "       0.64209086, 0.65189249])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0020553 , 0.00312245, 0.00204695, 0.001689  , 0.00335137,\n",
       "       0.00309493, 0.00289787, 0.00347016, 0.00280734, 0.00203748,\n",
       "       0.00216056, 0.00271549])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = selector.best_estimator_\n",
    "\n",
    "scaler_X = StandardScaler().fit(X_test)\n",
    "scaled_X = scaler_X.transform(X_test)\n",
    "scaled_X = pd.DataFrame(scaled_X,index=X_test.index)\n",
    "\n",
    "predict = best.predict(scaled_X)\n",
    "probs = best.predict_proba(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5593911347310342"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,probs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13716158, 0.04746452, 0.01001592, ..., 0.03384816, 0.02971134,\n",
       "       0.05862004])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
